{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Nets: Step by step\n",
    "\n",
    "Seth Weidman    \n",
    "\n",
    "06/15/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we're going to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What we're going to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Walk through this diagram and understand what is going on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/neural_net_basic.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's learn MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's learn MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  70000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 1: Randomly shuffle the images in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANJEX4gKcIPJEX4gaQIP5DU/wOtb/7Amp77jAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a667da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array(X[i], ndmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 2: multiply this image by the first set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V = np.random.randn(784, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.dot(x,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we have transformed the 784 dimensional images into 50 \"hidden features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wJO1tFyANE7awAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ba40860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(A[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**An intuition of what matrix multiplication does.**\n",
    "\n",
    "Let's say that each observation had three features $x_1$, $x_2$, and $x_3$, and we wanted to transform these three features into four hidden features, $a_1$, $a_2$, $a_3$, and $a_4$. How would we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we're transforming three features into four features, we'll use a 3 x 4 matrix to do this:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we would actually do the transformation as follows:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This shows concretely that _each one of the hidden features is just a linear combination of the original features of your data_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 3: feed these hidden features through the sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Refresher on the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only had four hidden features, the transformation would simply be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$ or\n",
    "\n",
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ b_4 = \\sigma(a_4) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "B = _sigmoid(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nif8DuNpFVUCFPZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12badf898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(B[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 4: multiply these sigmoided results by another matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(50, 10)\n",
    "C = np.dot(B,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.73036122,  2.40426708,  4.76437904,  0.4518629 , -3.73355318,\n",
       "       -1.46404718, -3.8039577 , -4.96813394, -1.81652786, -0.16892414])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bad7f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(C[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 4: multiply these sigmoided results by another matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, suppose that we had four hidden features, and we wanted to combine them down into one final prediction, with a second set of weights, $W$:\n",
    "\n",
    "$$ W = \\begin{bmatrix}w_{11} \\\\\n",
    "                      w_{21} \\\\\n",
    "                      w_{31} \\\\\n",
    "                      w_{41}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We would do it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ C = C(B, W) $$ or\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 5: Feed this through a sigmoid:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = _sigmoid(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically this is just:\n",
    "\n",
    "$ P_1 = \\sigma(c_1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXZIaZLhLUoMMd0lqkOEuSQ36X3NsMpeGM/cBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bc92518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(P[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9387946 ,  0.91715211,  0.99154393,  0.61108206,  0.0233495 ,\n",
       "        0.1878491 ,  0.02179673,  0.00690806,  0.13985102,  0.4578691 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Normalize P\n",
    "P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 5: Compute the loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, we'll compute mean squared error loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L_1 = \\frac{1}{2}(y - P_1)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(Y[i], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.40667647e-01,   4.20583998e-01,   4.91579685e-01,\n",
       "          1.86710644e-01,   4.76923097e-01,   1.76436422e-02,\n",
       "          2.37548642e-04,   2.38606685e-05,   9.77915447e-03,\n",
       "          1.04822057e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 6: Backpropogate, step 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial P} = -1.0 * (y - P) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9387946 ,  0.91715211,  0.99154393,  0.61108206, -0.9766505 ,\n",
       "         0.1878491 ,  0.02179673,  0.00690806,  0.13985102,  0.4578691 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdP = -1.0 * (y-P)\n",
    "dLdP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 7: Backpropogate, step 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digression on the sigmoid function:**\n",
    "\n",
    "If\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, since:\n",
    "\n",
    "$$ P = \\sigma(C) $$\n",
    "\n",
    "that means:\n",
    "\n",
    "$$ \\frac{\\partial P}{\\partial C} = \\sigma(C) * (1 - \\sigma(C)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by the chain rule (we'll use this over and over again):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial C} = \\frac{\\partial L}{\\partial P} * \\frac{\\partial P}{\\partial C} $$\n",
    "$$ \\frac{\\partial L}{\\partial C} =  -1.0 * (y-P) * \\sigma(C) * (1 - \\sigma(C)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.39424826e-02,   6.96889923e-02,   8.31366190e-03,\n",
       "          1.45230237e-01,  -2.22718340e-02,   2.86585998e-02,\n",
       "          4.64741718e-04,   4.73916751e-05,   1.68230594e-02,\n",
       "          1.13654552e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "dLdC = dLdP * dPdC\n",
    "dLdC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bd79f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dLdC[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 8: Backpropogate, step 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that, in our simple example with only four neurons in the hidden layer:\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$ \\begin{bmatrix}b_1 &&\n",
    "                  b_2 &&\n",
    "                  b_3 &&\n",
    "                  b_4\\end{bmatrix} * \\begin{bmatrix}w_{11} \\\\\n",
    "                  w_{21} \\\\\n",
    "                  w_{31} \\\\\n",
    "                  w_{41}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now recall that by \n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial W} $$\n",
    "\n",
    "we mean:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial C}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial C}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial C}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial C}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, looking at the formula for $C$, this is just \n",
    "\n",
    "$$ \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$\n",
    "                  \n",
    "or $B^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, then, we arrive at the elegant formula for the backpropogation at this stage:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial W} = B^T$$\n",
    "\n",
    "Similarly, we can show that:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial B} = W^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def hist_weight_update(update):\n",
    "    plt.hist(update.reshape(update.shape[0] * update.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bdee710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "dLdW = np.dot(dCdW, dLdC)\n",
    "plt.hist(dLdW.reshape(W.shape[0] * W.shape[1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 9: Backpropogate, step 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial B} = W^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bdcfba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "dLdB = np.dot(dLdC, dCdB)\n",
    "plt.hist(dLdB[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 10: backpropogate, step 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, since:\n",
    "\n",
    "$$ B = \\sigma(A) $$\n",
    "\n",
    "This means:\n",
    "\n",
    "$$ \\frac{\\partial B}{\\partial A} = \\sigma(A) * (1 - \\sigma(A)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fe5fb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "dLdA = dLdB * dBdA\n",
    "plt.hist(dLdA[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 11: backpropogate, step 6:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, recall that before, in our three input feature-four hidden feature example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we would actually do the transformation as follows:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in calculating:\n",
    "\n",
    "$$ \\frac{\\partial A}{\\partial V} $$\n",
    "\n",
    "which is really just syntax for:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial A}{\\partial v_{11}} & \\frac{\\partial A}{\\partial v_{12}} & \\frac{\\partial A}{\\partial v_{13}} & \\frac{\\partial A}{\\partial v_{14}} \\\\\n",
    "\\frac{\\partial A}{\\partial v_{21}} & \\frac{\\partial A}{\\partial v_{22}} & \\frac{\\partial A}{\\partial v_{23}} & \\frac{\\partial A}{\\partial v_{24}} \\\\\n",
    "\\frac{\\partial A}{\\partial v_{31}} & \\frac{\\partial A}{\\partial v_{32}} & \\frac{\\partial A}{\\partial v_{33}} & \\frac{\\partial A}{\\partial v_{34}} \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, note that focusing on just $a_1$ for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial a_1}{\\partial v_{11}} = x_1 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{21}} = x_2 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{31}} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas for $a_2$ and $a_3$\n",
    "\n",
    "$$ \\frac{\\partial a_2}{\\partial v_{11}} = 0 $$\n",
    "$$ \\frac{\\partial a_3}{\\partial v_{11}} = 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we write: \n",
    "    \n",
    "$$ A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$$ \\frac{\\partial A}{\\partial V} = \\begin{bmatrix}\n",
    "           \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} \\\\\n",
    "           \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} \\\\\n",
    "           \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} \\\\\n",
    "           \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "dLdV = np.dot(dAdV, dLdA)\n",
    "dLdV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSR2GgySpw3CQJHUYDpKkjv8PlfcUiagHs4YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb22cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_weight_update(dLdV);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 12: Update the weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(2)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Turning this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497640230.107162"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(X, Y, start_index, num_iter, V=None, W=None):\n",
    "    now = time.time()\n",
    "    np.random.seed(3)\n",
    "    if V is None:\n",
    "        V = np.random.randn(784, 50)\n",
    "    if W is None:\n",
    "        W = np.random.randn(50, 10)\n",
    "    for j in range(start_index, start_index + num_iter):\n",
    "        i = indices[j]\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y - P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    end = time.time()\n",
    "    print(\"# of seconds to do\",\n",
    "          num_iter,\n",
    "          \"iterations:\",\n",
    "          round(end - now, 2))\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(P, Y_test):\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of seconds to do 1000 iterations: 0.17\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.14\n",
      "# of seconds to do 1000 iterations: 0.19\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.22\n",
      "# of seconds to do 1000 iterations: 0.21\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_data = pd.DataFrame(index=[0], columns=['iterations', \n",
    "                                           'test_accuracy',\n",
    "                                           'train_accuracy'])\n",
    "train_size = X_train.shape[0]\n",
    "num_iter=1000\n",
    "num_iter_total = 0\n",
    "for i in range(int(train_size / num_iter)):\n",
    "    if i == 0:\n",
    "        V, W = learn(X_train, Y_train, num_iter_total, num_iter, V=None, W=None)\n",
    "    else:\n",
    "        V, W = learn(X_train, Y_train, num_iter_total, num_iter, V=V, W=W)\n",
    "        \n",
    "    P_test = predict(X_test, V, W)\n",
    "    P_train = predict(X_train, V, W)\n",
    "    test_accuracy = accuracy(P_test, Y_test)\n",
    "    train_accuracy = accuracy(P_train, Y_train)\n",
    "    num_iter_total += num_iter\n",
    "    df_data.loc[i, :] = [num_iter_total,\n",
    "                         test_accuracy,\n",
    "                         train_accuracy]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.365333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.465286</td>\n",
       "      <td>0.466016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.549714</td>\n",
       "      <td>0.548937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>0.613143</td>\n",
       "      <td>0.617317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.676143</td>\n",
       "      <td>0.676095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.680857</td>\n",
       "      <td>0.683444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7000</td>\n",
       "      <td>0.700143</td>\n",
       "      <td>0.705365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8000</td>\n",
       "      <td>0.737714</td>\n",
       "      <td>0.743492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9000</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0.82381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.833286</td>\n",
       "      <td>0.839095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11000</td>\n",
       "      <td>0.848429</td>\n",
       "      <td>0.85727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.84327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13000</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>0.858651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.835857</td>\n",
       "      <td>0.846222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.882286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.859429</td>\n",
       "      <td>0.860238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.883571</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18000</td>\n",
       "      <td>0.874429</td>\n",
       "      <td>0.879619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19000</td>\n",
       "      <td>0.885857</td>\n",
       "      <td>0.889286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.889143</td>\n",
       "      <td>0.896651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21000</td>\n",
       "      <td>0.874571</td>\n",
       "      <td>0.883524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22000</td>\n",
       "      <td>0.888429</td>\n",
       "      <td>0.890317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23000</td>\n",
       "      <td>0.884143</td>\n",
       "      <td>0.886143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24000</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.898857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.890286</td>\n",
       "      <td>0.896048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26000</td>\n",
       "      <td>0.888714</td>\n",
       "      <td>0.893857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.884857</td>\n",
       "      <td>0.890762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28000</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>0.904079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29000</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.901667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30000</td>\n",
       "      <td>0.892429</td>\n",
       "      <td>0.893651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34000</td>\n",
       "      <td>0.892429</td>\n",
       "      <td>0.898476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35000</td>\n",
       "      <td>0.894857</td>\n",
       "      <td>0.903889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36000</td>\n",
       "      <td>0.904857</td>\n",
       "      <td>0.909524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37000</td>\n",
       "      <td>0.905286</td>\n",
       "      <td>0.90946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38000</td>\n",
       "      <td>0.901857</td>\n",
       "      <td>0.908857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39000</td>\n",
       "      <td>0.894714</td>\n",
       "      <td>0.902619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40000</td>\n",
       "      <td>0.899429</td>\n",
       "      <td>0.906603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41000</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.904905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42000</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.891952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.906048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44000</td>\n",
       "      <td>0.898857</td>\n",
       "      <td>0.907905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45000</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>0.909444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.898873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47000</td>\n",
       "      <td>0.900143</td>\n",
       "      <td>0.911175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48000</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>0.904683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49000</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.914048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.905857</td>\n",
       "      <td>0.913857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51000</td>\n",
       "      <td>0.911857</td>\n",
       "      <td>0.917111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52000</td>\n",
       "      <td>0.912857</td>\n",
       "      <td>0.917587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53000</td>\n",
       "      <td>0.900714</td>\n",
       "      <td>0.91027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54000</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.91673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55000</td>\n",
       "      <td>0.913714</td>\n",
       "      <td>0.919032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56000</td>\n",
       "      <td>0.905143</td>\n",
       "      <td>0.905841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57000</td>\n",
       "      <td>0.904571</td>\n",
       "      <td>0.910603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58000</td>\n",
       "      <td>0.910429</td>\n",
       "      <td>0.919794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59000</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.914444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.919524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61000</td>\n",
       "      <td>0.909429</td>\n",
       "      <td>0.914746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62000</td>\n",
       "      <td>0.913571</td>\n",
       "      <td>0.917571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63000</td>\n",
       "      <td>0.913429</td>\n",
       "      <td>0.917365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterations test_accuracy train_accuracy\n",
       "0        1000      0.368571       0.365333\n",
       "1        2000      0.465286       0.466016\n",
       "2        3000      0.549714       0.548937\n",
       "3        4000      0.613143       0.617317\n",
       "4        5000      0.676143       0.676095\n",
       "5        6000      0.680857       0.683444\n",
       "6        7000      0.700143       0.705365\n",
       "7        8000      0.737714       0.743492\n",
       "8        9000      0.820857        0.82381\n",
       "9       10000      0.833286       0.839095\n",
       "10      11000      0.848429        0.85727\n",
       "11      12000         0.838        0.84327\n",
       "12      13000      0.849429       0.858651\n",
       "13      14000      0.835857       0.846222\n",
       "14      15000         0.877       0.882286\n",
       "15      16000      0.859429       0.860238\n",
       "16      17000      0.883571          0.884\n",
       "17      18000      0.874429       0.879619\n",
       "18      19000      0.885857       0.889286\n",
       "19      20000      0.889143       0.896651\n",
       "20      21000      0.874571       0.883524\n",
       "21      22000      0.888429       0.890317\n",
       "22      23000      0.884143       0.886143\n",
       "23      24000         0.894       0.898857\n",
       "24      25000      0.890286       0.896048\n",
       "25      26000      0.888714       0.893857\n",
       "26      27000      0.884857       0.890762\n",
       "27      28000      0.899857       0.904079\n",
       "28      29000      0.895286       0.901667\n",
       "29      30000      0.892429       0.893651\n",
       "..        ...           ...            ...\n",
       "33      34000      0.892429       0.898476\n",
       "34      35000      0.894857       0.903889\n",
       "35      36000      0.904857       0.909524\n",
       "36      37000      0.905286        0.90946\n",
       "37      38000      0.901857       0.908857\n",
       "38      39000      0.894714       0.902619\n",
       "39      40000      0.899429       0.906603\n",
       "40      41000         0.892       0.904905\n",
       "41      42000      0.886714       0.891952\n",
       "42      43000           0.9       0.906048\n",
       "43      44000      0.898857       0.907905\n",
       "44      45000      0.899857       0.909444\n",
       "45      46000         0.893       0.898873\n",
       "46      47000      0.900143       0.911175\n",
       "47      48000      0.898286       0.904683\n",
       "48      49000         0.911       0.914048\n",
       "49      50000      0.905857       0.913857\n",
       "50      51000      0.911857       0.917111\n",
       "51      52000      0.912857       0.917587\n",
       "52      53000      0.900714        0.91027\n",
       "53      54000      0.908571        0.91673\n",
       "54      55000      0.913714       0.919032\n",
       "55      56000      0.905143       0.905841\n",
       "56      57000      0.904571       0.910603\n",
       "57      58000      0.910429       0.919794\n",
       "58      59000         0.907       0.914444\n",
       "59      60000      0.914286       0.919524\n",
       "60      61000      0.909429       0.914746\n",
       "61      62000      0.913571       0.917571\n",
       "62      63000      0.913429       0.917365\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x127ad1c50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG5JwF0IINyThLoQQbkjCXQgh3ND/Bz3EaadHGO4vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129514400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_data.iterations, df_data.train_accuracy)\n",
    "plt.plot(df_data.iterations, df_data.test_accuracy);\n",
    "plt.legend(['Train accuracy', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural nets: what is going on?\n",
    "\n",
    "* Neural nets are nested functions, where the functions applied to the inputs alternate between two types:\n",
    "    * Linear functions, represented by matrix multiplications\n",
    "    * Non linear \"activation\" functions\n",
    "* Because they are functions, we can represent the neural net making a prediction mathematically. For example, if $X$ is the input, then the prediction $P$ is just: \n",
    "\n",
    "$$ P = D(C(B(A(X, V)), W)) $$\n",
    "\n",
    "where $A$, $B$, $C$, and $D$ are functions and $V$ and $W$ are weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review from last time (Pt. 2)\n",
    "\n",
    "* This prediction is compared to the actual value we were trying to predict, $Y$, and a loss is computed, for example:\n",
    "\n",
    "$$ L = (Y - P) ^ 2 $$\n",
    "\n",
    "* And then the weights are adjusted so that this loss will be reduced:\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key point\n",
    "\n",
    "* This process - of computing derivatives to continually update the weights in a neural net - works because of **the chain rule from calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can do basic neural nets!\n",
    "\n",
    "<img src=\"img/neural_net_check.png\">\n",
    "\n",
    "We got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Going deeper, each individual weight makes a contribution to the loss in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. If the loss of a neural net on a given observation $ X $ is: \n",
    "\n",
    "$$ L = L(D(C(B(A(X, V)), W))) $$\n",
    "\n",
    "we can use the chain rule the explicitly compute the loss with respect to each of the individual weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial C}{\\partial W} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ These are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial A}{\\partial V} * \\frac{\\partial B}{\\partial A} * \\frac{\\partial C}{\\partial B} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ To get all the weight updates to line up correctly, some of these are elementwise multiplications and some of these are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key point:** Each one of these individual computations is actually quite simple. I cover what each one of these is in a blog post [here](http://sethweidman.com/neural_net_post_2) and a SlideShare presentation [here](https://www.slideshare.net/SethHWeidman/neural-nets-from-scratch-72835271)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Read in the MNIST Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  70000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Visualize the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADnBJREFUeJzt3X+sVPWZx/HPoy3+IZigXMmNyN5uJU2MiYAjWVOiXbpU\nMURsTBSilY1aiFbdRjQa9o8lyh8ErQ2JayNdSbmk0pqAQpTs1iX+SBOtDHgRrV1xzW2A8OOiDUg0\nsMKzf9xDc6v3fGeYOTNn7n3er+TmzpznnHuejH44M+d75nzN3QUgnrPKbgBAOQg/EBThB4Ii/EBQ\nhB8IivADQRF+ICjCDwRF+IGgvtHOnU2YMMF7enrauUsglP7+fh0+fNjqWbep8JvZdZJWSTpb0n+4\n+4rU+j09PapWq83sEkBCpVKpe92G3/ab2dmS/l3SHEmXSlpgZpc2+vcAtFczn/lnSPrI3T929xOS\nfiNpXjFtAWi1ZsJ/kaQ9Q57vzZb9DTNbZGZVM6sODAw0sTsARWr52X53X+3uFXevdHV1tXp3AOrU\nTPj3Sbp4yPNJ2TIAI0Az4d8maYqZfcvMxkiaL2lzMW0BaLWGh/rc/Uszu1fSf2lwqG+Nu79fWGcA\nWqqpcX533yJpS0G9AGgjLu8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKi2TtGN0Wf79u3J+lNPPZVb6+3tTW57\n++23J+v33Xdfsj59+vRkPTqO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFPj/GbWL+kzSSclfenu\nlSKaQufo6+tL1mfPnp2sHz16NLdmZslt161bl6xv3rw5Wf/kk0+S9eiKuMjnH939cAF/B0Ab8bYf\nCKrZ8Luk35nZdjNbVERDANqj2bf9M919n5ldKOkVM/uTu78xdIXsH4VFkjR58uQmdwegKE0d+d19\nX/b7kKQXJM0YZp3V7l5x90pXV1czuwNQoIbDb2bnmtm4048l/UDSe0U1BqC1mnnbP1HSC9lwzTck\nPefu/1lIVwBaruHwu/vHki4vsBeU4O23307Wb7rppmT9yJEjyXpqLH/cuHHJbceMGZOs1xrHf/PN\nN3NrV1xxRVP7Hg0Y6gOCIvxAUIQfCIrwA0ERfiAowg8Exa27R4HPP/88t7Zjx47ktrfddluyvn//\n/oZ6qscll1ySrD/88MPJ+vz585P1mTNn5tYee+yx5LZLly5N1kcDjvxAUIQfCIrwA0ERfiAowg8E\nRfiBoAg/EBTj/KPA4sWLc2vr169vYydn5p133knWjx07lqxfffXVyfrrr7+eW9u1a1dy2wg48gNB\nEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzjwDbt29P1l9++eXcmrs3te9rrrkmWZ87d26y/tBDD+XW\nuru7k9tOmzYtWR8/fnyy/uqrr+bWmn1dRgOO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVM1xfjNb\nI2mupEPuflm27HxJv5XUI6lf0s3u/pfWtTm69fX1JeuzZ89O1o8ePZpbS02RLUlz5sxJ1mvdD+C1\n115L1pcvX55bu+uuu5LbdnV1JeuXX56eIf6ss/KPbalrI6Ta8x1Mnz49WR8J6jny/0rSdV9Z9oik\nre4+RdLW7DmAEaRm+N39DUmffmXxPElrs8drJd1YcF8AWqzRz/wT3f30PE4HJE0sqB8AbdL0CT8f\nvEg690JpM1tkZlUzqw4MDDS7OwAFaTT8B82sW5Ky34fyVnT31e5ecfdKrRM4ANqn0fBvlrQwe7xQ\n0qZi2gHQLjXDb2brJb0p6TtmttfM7pS0QtJsM9st6Z+y5wBGkJrj/O6+IKf0/YJ7GbU+/PDDZH3l\nypXJ+pEjR5L1CRMm5NZqfWd+4cKFyfrYsWOT9Vrf569VL8sXX3yRrD/xxBPJ+nPPPVdkO6XgCj8g\nKMIPBEX4gaAIPxAU4QeCIvxAUNy6uwDHjx9P1h988MFkfcuWLcn6uHHjkvXe3t7cWqVSSW5ba8gr\nqj179pTdQstx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnL0Ct2zzXGsevZdOm9L1Sak2jDQyH\nIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4fwEeeOCBZH1wRrN8tcbpGcdvzKlTp3Jrqem7pdr/\nzUYDjvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTNcX4zWyNprqRD7n5ZtmyZpB9LGshWW+ruzX1p\nvcO99NJLubWdO3cmtzWzZP2GG25oqCekpcbya/03mTp1atHtdJx6jvy/knTdMMt/7u5Ts59RHXxg\nNKoZfnd/Q9KnbegFQBs185n/XjN718zWmNn4wjoC0BaNhv8Xkr4taaqk/ZJ+lreimS0ys6qZVQcG\nBvJWA9BmDYXf3Q+6+0l3PyXpl5JmJNZd7e4Vd690dXU12ieAgjUUfjPrHvL0h5LeK6YdAO1Sz1Df\neknfkzTBzPZK+jdJ3zOzqZJcUr+kxS3sEUAL1Ay/uy8YZvGzLeilo6XmsT9x4kRy2wsvvDBZv+WW\nWxrqabQ7fvx4sr5s2bKG//asWbOS9RUrVjT8t0cKrvADgiL8QFCEHwiK8ANBEX4gKMIPBMWtu9vg\nnHPOSda7u7uT9dGq1lDe8uXLk/XHH388WZ80aVJubcmSJcltx44dm6yPBhz5gaAIPxAU4QeCIvxA\nUIQfCIrwA0ERfiAoxvnbIPKtufv6+nJrK1euTG77/PPPJ+u1XteNGzcm69Fx5AeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoBjnr5O7N1STpBdffDFZX7VqVUM9dYInn3wyWU99J//IkSPJbW+99dZkvbe3\nN1lHGkd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq5ji/mV0sqVfSREkuabW7rzKz8yX9VlKPpH5J\nN7v7X1rXarnMrKGaJB04cCBZv//++5P1O+64I1m/4IILcmtvvfVWctt169Yl6zt37kzW9+7dm6xP\nnjw5t3bttdcmt73nnnuSdTSnniP/l5KWuPulkv5B0k/M7FJJj0ja6u5TJG3NngMYIWqG3933u/uO\n7PFnkj6QdJGkeZLWZqutlXRjq5oEULwz+sxvZj2Spkn6g6SJ7r4/Kx3Q4McCACNE3eE3s7GSNkj6\nqbsfHVrzwYvbh73A3cwWmVnVzKoDAwNNNQugOHWF38y+qcHg/9rdT98V8aCZdWf1bkmHhtvW3Ve7\ne8XdK11dXUX0DKAANcNvg6eyn5X0gbsP/QrXZkkLs8cLJW0qvj0ArVLPV3q/K+lHknaZ2en7MC+V\ntELS82Z2p6Q/S7q5NS2OfCdPnkzWn3766WR9w4YNyfp5552XW9u9e3dy22ZdddVVyfqsWbNya48+\n+mjR7eAM1Ay/u/9eUt5A9veLbQdAu3CFHxAU4QeCIvxAUIQfCIrwA0ERfiAobt1dp9R49pVXXpnc\ndtu2bU3tu9ZXgg8ePNjw3059HViS5s+fn6yP5NuOR8eRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC\nYpy/TpMmTcqtbdy4MbcmSc8880yynprGulm1bgt+9913J+tTpkwpsh10EI78QFCEHwiK8ANBEX4g\nKMIPBEX4gaAIPxCUDc601R6VSsWr1Wrb9gdEU6lUVK1W03PGZzjyA0ERfiAowg8ERfiBoAg/EBTh\nB4Ii/EBQNcNvZheb2atm9kcze9/M/iVbvszM9plZX/ZzfevbBVCUem7m8aWkJe6+w8zGSdpuZq9k\ntZ+7+xOtaw9Aq9QMv7vvl7Q/e/yZmX0g6aJWNwagtc7oM7+Z9UiaJukP2aJ7zexdM1tjZuNztllk\nZlUzqw4MDDTVLIDi1B1+MxsraYOkn7r7UUm/kPRtSVM1+M7gZ8Nt5+6r3b3i7pWurq4CWgZQhLrC\nb2bf1GDwf+3uGyXJ3Q+6+0l3PyXpl5JmtK5NAEWr52y/SXpW0gfu/uSQ5d1DVvuhpPeKbw9Aq9Rz\ntv+7kn4kaZeZ9WXLlkpaYGZTJbmkfkmLW9IhgJao52z/7yUN9/3gLcW3A6BduMIPCIrwA0ERfiAo\nwg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFun6DazAUl/HrJogqTDbWvg\nzHRqb53al0RvjSqyt79z97rul9fW8H9t52ZVd6+U1kBCp/bWqX1J9NaosnrjbT8QFOEHgio7/KtL\n3n9Kp/bWqX1J9NaoUnor9TM/gPKUfeQHUJJSwm9m15nZ/5jZR2b2SBk95DGzfjPblc08XC25lzVm\ndsjM3huy7Hwze8XMdme/h50mraTeOmLm5sTM0qW+dp0243Xb3/ab2dmSPpQ0W9JeSdskLXD3P7a1\nkRxm1i+p4u6ljwmb2dWSjknqdffLsmUrJX3q7iuyfzjHu/vDHdLbMknHyp65OZtQpnvozNKSbpT0\nzyrxtUv0dbNKeN3KOPLPkPSRu3/s7ick/UbSvBL66Hju/oakT7+yeJ6ktdnjtRr8n6ftcnrrCO6+\n3913ZI8/k3R6ZulSX7tEX6UoI/wXSdoz5PleddaU3y7pd2a23cwWld3MMCZm06ZL0gFJE8tsZhg1\nZ25up6/MLN0xr10jM14XjRN+XzfT3adLmiPpJ9nb247kg5/ZOmm4pq6Zm9tlmJml/6rM167RGa+L\nVkb490m6eMjzSdmyjuDu+7LfhyS9oM6bffjg6UlSs9+HSu7nrzpp5ubhZpZWB7x2nTTjdRnh3yZp\nipl9y8zGSJovaXMJfXyNmZ2bnYiRmZ0r6QfqvNmHN0tamD1eKGlTib38jU6ZuTlvZmmV/Np13IzX\n7t72H0nXa/CM//9K+tcyesjp6+8l7cx+3i+7N0nrNfg28P80eG7kTkkXSNoqabek/5Z0fgf1tk7S\nLknvajBo3SX1NlODb+nfldSX/Vxf9muX6KuU140r/ICgOOEHBEX4gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiCo/wfKBnT2y+G31wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ae8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Train the neural net for one epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learn(X, Y, num_iter):\n",
    "    np.random.seed(2)\n",
    "    V = np.random.randn(784, 50)\n",
    "    W = np.random.randn(50, 10)\n",
    "    for j in range(num_iter):\n",
    "        i = np.random.randint(0, num_iter)\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        sum_P = np.sum(P)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y-P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V, W = learn(X_train, Y_train, num_iter=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = predict(X_test, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 91.3 percent\n"
     ]
    }
   ],
   "source": [
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "91.3% accuracy. Not bad for a naive approach with zero tricks. However..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...it's far from optimal: [see here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "\n",
    "<img src='img/MNIST_performance.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building deeper nets\n",
    "* Changing \"learning rates\" / using \"learning rate decay\"\n",
    "* Adding \"dropout\"\n",
    "* ...and that doesn't even cover many of the cutting edge techniques listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And of course, we have to build it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**\"What I cannot build, I cannot understand.\"**\n",
    "\n",
    "--Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before:\n",
    "\n",
    "With a neural net with one hidden layer, one pass through was 25 manually coded steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_This will quickly get unweildy if we add more hidden layers._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New understanding: \"Layers\"\n",
    "\n",
    "To do \"Deep Learning\" you have to stop thinking of neural nets as a series of functions $ L = L(D(C(B(A(X, V)), W)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New understanding: \"Layers\"\n",
    "\n",
    "Instead, we'll think of them as a series of layers:\n",
    "\n",
    "Slide from March talk:\n",
    "\n",
    "<img src='img/neural_net_layers.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll define a `NeuralNetwork` class that defines a neural network as a series of `Layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Props\n",
    "\n",
    "<img src=\"img/andersbll.png\">\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers - and don't forget about that activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        s = sigmoid(x)\n",
    "        return s*(1-s)\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running this neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "layer2 = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net_pass(net, x, y):\n",
    "    pred = net.forwardpass(x)\n",
    "    loss = net.loss(pred, y)\n",
    "    net.backpropogate(loss)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(4)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through every element in the training set: \n",
    "for index in indices:\n",
    "    x = np.array(X_train[index], ndmin=2)\n",
    "    y = np.array(Y_train[index], ndmin=2)\n",
    "    neural_net_pass(nn_mnist, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 82.3 percent\n"
     ]
    }
   ],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A different random weight initialization gave us just 82.3% accuracy this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network\n",
    "\n",
    "#### Turning what we did above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deeper networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=75, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=75, n_out=50, activation_function=sigmoid)\n",
    "hidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_3 = Linear(n_in=50, n_out=25, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=25, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_accuracy_net_layers(layers):\n",
    "    # Define net\n",
    "    net = NeuralNetwork(\n",
    "        layers=layers,\n",
    "        random_seed=2)\n",
    "\n",
    "    accuracy_one_hidden = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    return accuracy_one_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with one hidden layer was 82.3\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with one hidden layer was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with two hidden layers was 90.7\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with two hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with three hidden layers was 88.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with three hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with four hidden layers was 87.9\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      hidden_3,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with four hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**\"Deep Learning\" can help, but we need more \"tricks\" to really get these things working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', self.learning_rate)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    print(\"The accuracy of a network with learning rate\", learning_rate, \"was\", np.round(accuracy_lr * 100.0, 1))\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because backpropogation involves multiplying a value by the derivative of the activation function, gradients (that tell the weights how to update) get smaller and smaller as you go get further from the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sigmoid_deriv_trask.png\">\n",
    "**At most, the gradient can be multiplied by 0.25 at each layer.** More [here](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, \n",
    "                 random_seed, \n",
    "                 learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', \n",
    "                    self.learning_rate/(10.0 ** i))\n",
    "            layer.initialize_weights()\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer is\", \n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accuracy is up to 91.8%, up from 90.7% before. We could also view that as roughly a 10% reduction in error rate. \n",
    "\n",
    "**Having a higher learning rate in earlier layers vs. later layers does help accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights in a neural net are updated according to:\n",
    "\n",
    "$$ W =  W - \\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that this is equivalent to doing gradient descent with each parameter:\n",
    "\n",
    "<img src=\"img/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is analogous to a ball rolling down a hill. \n",
    "\n",
    "Balls rolling down hills have momentum. So, therefore, should our weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum\n",
    "\n",
    "Let's define our weight update $ \\frac{\\partial L}{\\partial W} $ to be $ U_t $. Then, instead of our weight update being $ U_t $ at each time step, it will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ U_t + \\mu * U_{t-1} + \\mu^2 * U_{t-2} + ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\mu$ is a decay parameter between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to, and often described as, increasing your learning rate when your weight updates are going in the same direction, iteration after iteration, and lowering your learning rate when the opposite is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = None\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate, momentum):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate, momentum):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer, and \\n momentum\", momentum, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [1]\n",
    "momentum = [0.5, 0.75, 0.9]\n",
    "for learning_rate in learning_rates:\n",
    "    for m in momentum:\n",
    "        learning_rate_accuracy(learning_rate, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we get a practical lesson: **throwing the \"kitchen sink\" of neural net tricks at a problem in unnecessary at best and actually harmful at worst.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Indeed, learning rate momentum has been shown to work on Recurrent Neural Nets, not fully connected neural nets as we have here. See \n",
    "[Bengio et. al. (2014)](https://arxiv.org/pdf/1212.0901v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout can help prevent neural networks from overfitting. It involves \"dropping\" a portion of the neurons - that is, setting their values to zero - on each forward pass through the network. \n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "This nudges the network toward learning \"redundant representations of its data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, dropout):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.dropout = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_dropout(dropout):\n",
    "    \n",
    "    net = NeuralNetworkDropout(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        dropout = dropout\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with dropout\", dropout, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dropouts = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_dropout(x) for x in dropouts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Adding dropout did _not_ improve accuracy.** Our network was not overfitting in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Last one, just for fun: DropConnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we saw above, the highest performance model on the MNIST data involved \"Drop Connect\", where a portion of the _weights_ in the neural net are set to zero, as opposed to half the _neurons_.\n",
    "\n",
    "[Not in TensorFlow!](https://stackoverflow.com/questions/37135885/dropconnect-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, drop_connect,\n",
    "                 dropout=None):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.drop_connect = drop_connect\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "            setattr(layer, 'drop_connect', self.drop_connect)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    drop_connect = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.activation_input = np.dot(layer_input, \n",
    "                                           drop_connected_weights)\n",
    "        else:\n",
    "            self.activation_input = np.dot(layer_input, self.W)\n",
    "        self.iteration += 1\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.drop_connect = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_drop_connect(drop_connect):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        drop_connect = drop_connect\n",
    "    )\n",
    "\n",
    "    accuracy_lr = train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with drop_connect\", \n",
    "          drop_connect, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "drop_connect_values = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_drop_connect(x) for x in drop_connect_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This network, not being convolutional, is likely _underfitting_ rather than _overfitting_ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a lot of \"tricks\" to improve the training of neural nets. **Now you know not just conceptually what those tricks are doing, but how to implement them.** You may even be able to implement a few of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll examine:\n",
    "* Different activation functions! (here we only used boring ol' sigmoid)\n",
    "* Different weight initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Convolutional layers (yes, from scratch)\n",
    "* Recurrent neural nets (including LSTMs, omg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next steps for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go to [the GitHub repo for this talk](https://github.com/SethHWeidman/neural_net_talks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Star, fork, and contribute!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

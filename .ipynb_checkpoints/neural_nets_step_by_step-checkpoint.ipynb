{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Nets: Step by step\n",
    "\n",
    "Seth Weidman    \n",
    "\n",
    "06/15/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we're going to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What we're going to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Walk through this diagram and understand what is going on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/neural_net_basic.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's learn MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's learn MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  70000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 1: Randomly shuffle the images in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANJEX4gKcIPJEX4gaQIP5DU/wOtb/7Amp77jAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12757fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array(X[i], ndmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 2: multiply this image by the first set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V = np.random.randn(784, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.dot(x,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we have transformed the 784 dimensional images into 50 \"hidden features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFrokNcJCl6RG/C9/NPXgXPj5cQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127867b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(A[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**An intuition of what matrix multiplication does.**\n",
    "\n",
    "Let's say that each observation had three features $x_1$, $x_2$, and $x_3$, and we wanted to transform these three features into four hidden features, $a_1$, $a_2$, $a_3$, and $a_4$. How would we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we're transforming three features into four features, we'll use a 3 x 4 matrix to do this:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we would actually do the transformation as follows:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This shows concretely that _each one of the hidden features is just a linear combination of the original features of your data_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 3: feed these hidden features through the sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Refresher on the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ ... $$\n",
    "$$ b_{50} = \\sigma(a_{50}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12785e828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(B[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 4: multiply these sigmoided results by another matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(50, 10)\n",
    "C = np.dot(B,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.38746124,  8.51271821, -2.78804917,  1.71200362,  3.07822872,\n",
       "       -1.34238294,  3.63447178, -0.63555907,  1.85311768, -0.71858372])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1CDjLkkNMu6S1CDjLkkN+j8RV4romQty5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127397160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(C[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 5: Feed this through a sigmoid:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = _sigmoid(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEARJREFUeJzt3X2MZXV9x/H3x931oZFC404j2QfHRkyKVAUnFGPSUqkN\notn9Q2wg8QGD3YRI1da0AZtgpf9ImmpjMdKtEEGtYtGYUZYYUjFoU1YHXFYetNlaKktJGAEXCYJd\n/faPe2Km4wz3zMyZufDj/Upu9jx855zvPZn53LO/e+65qSokSW151qQbkCQNz3CXpAYZ7pLUIMNd\nkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjzpHa8devWmp6entTuJelp6dZbb/1RVU2Nq5tYuE9P\nTzM3Nzep3UvS01KS/+5T57CMJDXIcJekBhnuktQgw12SGmS4S1KDeod7kk1JvpPkK0use06Sa5Mc\nSrI/yfSQTUqSVmYlZ+7vAe5eZt35wMNV9RLgI8Bla21MkrR6vcI9yXbgDcAnlinZDVzdTV8HnJEk\na29PkrQafc/c/x74S+AXy6zfBtwLUFVHgSPAC9bcnSRpVcZ+QjXJG4EHqurWJKevZWdJ9gB7AHbu\n3LmWTUnSmkxfdP3E9n3Ph96w7vvoc+b+GmBXknuAzwGvTfLpRTX3ATsAkmwGjgUeXLyhqtpbVTNV\nNTM1NfbWCJKkVRob7lV1cVVtr6pp4Bzga1X1lkVls8Dbu+mzu5oatFNJUm+rvnFYkkuBuaqaBa4E\nPpXkEPAQoxcBSdKErCjcq+rrwNe76UsWLH8cePOQjUmSVs9PqEpSgwx3SWqQ4S5JDTLcJalBhrsk\nNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KD\nxoZ7kucm+VaS25PcmeSDS9Scl2Q+yYHu8c71aVeS1Eefr9l7AnhtVT2aZAvwzSQ3VNUti+quraoL\nh29RkrRSY8O9qgp4tJvd0j1qPZuSJK1NrzH3JJuSHAAeAG6sqv1LlL0pycEk1yXZMWiXkqQV6RXu\nVfXzqnolsB04NclJi0q+DExX1cuBG4Grl9pOkj1J5pLMzc/Pr6VvSdKTWNHVMlX1Y+Am4MxFyx+s\nqie62U8Ar1rm5/dW1UxVzUxNTa2mX0lSD32ulplKclw3/TzgdcD3FtUcv2B2F3D3kE1Kklamz9Uy\nxwNXJ9nE6MXg81X1lSSXAnNVNQu8O8ku4CjwEHDeejUsSRqvz9UyB4GTl1h+yYLpi4GLh21NkrRa\nfkJVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y\n3CWpQYa7JDXIcJekBhnuktQgw12SGtTnO1Sfm+RbSW5PcmeSDy5R85wk1yY5lGR/kun1aFaS1E+f\nM/cngNdW1SuAVwJnJjltUc35wMNV9RLgI8Blw7YpSVqJseFeI492s1u6Ry0q2w1c3U1fB5yRJIN1\nKUlakV5j7kk2JTkAPADcWFX7F5VsA+4FqKqjwBHgBUtsZ0+SuSRz8/Pza+tckrSsXuFeVT+vqlcC\n24FTk5y0mp1V1d6qmqmqmampqdVsQpLUw4qulqmqHwM3AWcuWnUfsAMgyWbgWODBIRqUJK1cn6tl\nppIc100/D3gd8L1FZbPA27vps4GvVdXicXlJ0gbZ3KPmeODqJJsYvRh8vqq+kuRSYK6qZoErgU8l\nOQQ8BJyzbh1LksYaG+5VdRA4eYnllyyYfhx487CtSZJWy0+oSlKDDHdJapDhLkkNMtwlqUGGuyQ1\nyHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoP6\nfIfqjiQ3JbkryZ1J3rNEzelJjiQ50D0uWWpbkqSN0ec7VI8C76uq25IcA9ya5MaqumtR3Teq6o3D\ntyhJWqmxZ+5VdX9V3dZN/wS4G9i23o1JklZvRWPuSaYZfVn2/iVWvzrJ7UluSPKyZX5+T5K5JHPz\n8/MrblaS1E/vcE/yfOALwHur6pFFq28DXlRVrwD+AfjSUtuoqr1VNVNVM1NTU6vtWZI0Rq9wT7KF\nUbB/pqq+uHh9VT1SVY920/uALUm2DtqpJKm3PlfLBLgSuLuqPrxMzQu7OpKc2m33wSEblST11+dq\nmdcAbwW+m+RAt+z9wE6AqroCOBu4IMlR4KfAOVVV69CvJKmHseFeVd8EMqbmcuDyoZqSJK2Nn1CV\npAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq\nkOEuSQ0y3CWpQYa7JDXIcJekBvX5DtUdSW5KcleSO5O8Z4maJPlokkNJDiY5ZX3alST10ec7VI8C\n76uq25IcA9ya5MaqumtBzeuBE7rH7wIf7/6VJE3A2DP3qrq/qm7rpn8C3A1sW1S2G7imRm4Bjkty\n/ODdSpJ66XPm/ktJpoGTgf2LVm0D7l0wf7hbdv+in98D7AHYuXPnyjqV1KTpi66fdAtN6v2GapLn\nA18A3ltVj6xmZ1W1t6pmqmpmampqNZuQJPXQK9yTbGEU7J+pqi8uUXIfsGPB/PZumSRpAvpcLRPg\nSuDuqvrwMmWzwNu6q2ZOA45U1f3L1EqS1lmfMffXAG8FvpvkQLfs/cBOgKq6AtgHnAUcAh4D3jF8\nq5KkvsaGe1V9E8iYmgLeNVRTkqS18ROqktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCX\npAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9fmavauSPJDkjmXWn57kSJID\n3eOS4duUJK1En6/Z+yRwOXDNk9R8o6reOEhHkqQ1G3vmXlU3Aw9tQC+SpIEMNeb+6iS3J7khycsG\n2qYkaZX6DMuMcxvwoqp6NMlZwJeAE5YqTLIH2AOwc+fOAXYtSVrKms/cq+qRqnq0m94HbEmydZna\nvVU1U1UzU1NTa921JGkZaw73JC9Mkm761G6bD651u5Kk1Rs7LJPks8DpwNYkh4EPAFsAquoK4Gzg\ngiRHgZ8C51RVrVvHkqSxxoZ7VZ07Zv3ljC6VlCQ9RfgJVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnu\nktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQ2HBP\nclWSB5Lcscz6JPlokkNJDiY5Zfg2JUkr0efM/ZPAmU+y/vXACd1jD/DxtbclSVqLseFeVTcDDz1J\nyW7gmhq5BTguyfFDNShJWrkhxty3AfcumD/cLZMkTcjmjdxZkj2Mhm7YuXPnqrczfdH1Q7Wkp7B7\nPvSGSbcgPW0NceZ+H7Bjwfz2btmvqKq9VTVTVTNTU1MD7FqStJQhwn0WeFt31cxpwJGqun+A7UqS\nVmnssEySzwKnA1uTHAY+AGwBqKorgH3AWcAh4DHgHevVrCSpn7HhXlXnjllfwLsG60iStGZ+QlWS\nGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalB\nhrskNchwl6QGGe6S1CDDXZIa1Cvck5yZ5PtJDiW5aIn15yWZT3Kge7xz+FYlSX31+Q7VTcDHgNcB\nh4FvJ5mtqrsWlV5bVReuQ4+SpBXqc+Z+KnCoqn5QVT8DPgfsXt+2JElr0SfctwH3Lpg/3C1b7E1J\nDia5LsmOQbqTJK3KUG+ofhmYrqqXAzcCVy9VlGRPkrkkc/Pz8wPtWpK0WJ9wvw9YeCa+vVv2S1X1\nYFU90c1+AnjVUhuqqr1VNVNVM1NTU6vpV5LUQ59w/zZwQpIXJ3k2cA4wu7AgyfELZncBdw/XoiRp\npcZeLVNVR5NcCHwV2ARcVVV3JrkUmKuqWeDdSXYBR4GHgPPWsWdJ0hhjwx2gqvYB+xYtu2TB9MXA\nxcO2JklaLT+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrsk\nNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qFe5Jzkzy/SSHkly0xPrnJLm2W78/yfTQjUqS\n+hsb7kk2AR8DXg+cCJyb5MRFZecDD1fVS4CPAJcN3agkqb8+Z+6nAoeq6gdV9TPgc8DuRTW7gau7\n6euAM5JkuDYlSSvRJ9y3AfcumD/cLVuypqqOAkeAFwzRoCRp5TZv5M6S7AH2dLOPJvn+Ru7/KWAr\n8KNJNzFhvY9B2h3ce6b/HjzTnz+5bE3H4EV9ivqE+33AjgXz27tlS9UcTrIZOBZ4cPGGqmovsLdP\nYy1KMldVM5PuY5I8Bh6DZ/rzh405Bn2GZb4NnJDkxUmeDZwDzC6qmQXe3k2fDXytqmq4NiVJKzH2\nzL2qjia5EPgqsAm4qqruTHIpMFdVs8CVwKeSHAIeYvQCIEmakF5j7lW1D9i3aNklC6YfB948bGtN\nesYOSS3gMfAYPNOfP2zAMYijJ5LUHm8/IEkNMtzXQY/bNfx5kruSHEzyr0l6Xdr0dDLuGCyoe1OS\nStLU1RN9nn+SP+5+D+5M8s8b3eN66/F3sDPJTUm+0/0tnDWJPtdLkquSPJDkjmXWJ8lHu+NzMMkp\ngzZQVT4GfDB60/k/gd8Cng3cDpy4qOYPgF/rpi8Arp103xt9DLq6Y4CbgVuAmUn3vcG/AycA3wF+\no5v/zUn3PYFjsBe4oJs+Ebhn0n0PfAx+DzgFuGOZ9WcBNwABTgP2D7l/z9yHN/Z2DVV1U1U91s3e\nwuizAy3pc8sKgL9hdB+ixzeyuQ3Q5/n/CfCxqnoYoKoe2OAe11ufY1DAr3fTxwL/s4H9rbuqupnR\n1YPL2Q1cUyO3AMclOX6o/Rvuw+tzu4aFzmf06t2Ssceg+y/ojqq6fiMb2yB9fgdeCrw0yb8luSXJ\nmRvW3cbocwz+GnhLksOMrsb7041p7SljpVmxIht6+wH9f0neAswAvz/pXjZSkmcBHwbOm3Ark7SZ\n0dDM6Yz+53Zzkt+pqh9PtKuNdS7wyar6uySvZvRZmZOq6heTbqwFnrkPr8/tGkjyh8BfAbuq6okN\n6m2jjDsGxwAnAV9Pcg+j8cbZht5U7fM7cBiYrar/rar/Av6DUdi3os8xOB/4PEBV/TvwXEb3nXmm\n6JUVq2W4D2/s7RqSnAz8I6Ngb22sFcYcg6o6UlVbq2q6qqYZve+wq6rmJtPu4PrcsuNLjM7aSbKV\n0TDNDzayyXXW5xj8EDgDIMlvMwr3+Q3tcrJmgbd1V82cBhypqvuH2rjDMgOrfrdr+Fvg+cC/dLe9\n/2FV7ZpY0wPreQya1fP5fxX4oyR3AT8H/qKqfuVme09XPY/B+4B/SvJnjN5cPa+6y0hakOSzjF7A\nt3bvK3wA2AJQVVcwep/hLOAQ8BjwjkH339CxlCR1HJaRpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5J\nDTLcJalBhrskNej/ACa7Zr+WeloNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127bf1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(P[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96731036,  0.99979914,  0.0579734 ,  0.84709598,  0.95598571,\n",
       "        0.20711846,  0.97428105,  0.34625111,  0.86449274,  0.32770493])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TODO: Normalize P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 5: Compute the loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(Y[i], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46784467,  0.49979916,  0.00168046,  0.01168982,  0.45695434,\n",
       "         0.02144903,  0.47461178,  0.05994491,  0.37367385,  0.05369526]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 6: Backpropogate, step 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96731036,  0.99979914,  0.0579734 , -0.15290402,  0.95598571,\n",
       "         0.20711846,  0.97428105,  0.34625111,  0.86449274,  0.32770493]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdP = -1.0 * (y-P)\n",
    "dLdP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural net, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 7: Backpropogate, step 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03058735,  0.00020078,  0.00316607, -0.0198048 ,  0.04022504,\n",
       "         0.03401308,  0.02441303,  0.07837784,  0.10127104,  0.07219812]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "dLdC = dLdP * dPdC\n",
    "dLdC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHdJapDhLkkNMtwlqUH/A2EKMHn319wfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127756828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dLdC[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Step 8: Backpropogate, step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_weight_update(update):\n",
    "    plt.hist(update.reshape(update.shape[0] * update.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12776d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "dLdW = np.dot(dCdW, dLdC)\n",
    "plt.hist(dLdW.reshape(W.shape[0] * W.shape[1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Step 9: Backpropogate, step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127b915c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "dLdB = np.dot(dLdC, dCdB)\n",
    "plt.hist(dLdB[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: backpropogate, step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127b91710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "dLdA = dLdB * dBdA\n",
    "plt.hist(dLdA[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: backpropogate, step 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "dLdV = np.dot(dAdV, dLdA)\n",
    "dLdV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127fffe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_weight_update(dLdV);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Step 12: Update the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(2)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Turning this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497640230.107162"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X, Y, start_index, num_iter, V=None, W=None):\n",
    "    now = time.time()\n",
    "    np.random.seed(3)\n",
    "    if V is None:\n",
    "        V = np.random.randn(784, 50)\n",
    "    if W is None:\n",
    "        W = np.random.randn(50, 10)\n",
    "    for j in range(start_index, start_index + num_iter):\n",
    "        i = indices[j]\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y - P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    end = time.time()\n",
    "    print(\"# of seconds to do\",\n",
    "          num_iter,\n",
    "          \"iterations:\",\n",
    "          round(end - now, 2))\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(P, Y_test):\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of seconds to do 1000 iterations: 0.17\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.14\n",
      "# of seconds to do 1000 iterations: 0.19\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.22\n",
      "# of seconds to do 1000 iterations: 0.21\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.13\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.11\n",
      "# of seconds to do 1000 iterations: 0.12\n",
      "# of seconds to do 1000 iterations: 0.13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_data = pd.DataFrame(index=[0], columns=['iterations', \n",
    "                                           'test_accuracy',\n",
    "                                           'train_accuracy'])\n",
    "train_size = X_train.shape[0]\n",
    "num_iter=1000\n",
    "num_iter_total = 0\n",
    "for i in range(int(train_size / num_iter)):\n",
    "    if i == 0:\n",
    "        V, W = learn(X_train, Y_train, num_iter_total, num_iter, V=None, W=None)\n",
    "    else:\n",
    "        V, W = learn(X_train, Y_train, num_iter_total, num_iter, V=V, W=W)\n",
    "        \n",
    "    P_test = predict(X_test, V, W)\n",
    "    P_train = predict(X_train, V, W)\n",
    "    test_accuracy = accuracy(P_test, Y_test)\n",
    "    train_accuracy = accuracy(P_train, Y_train)\n",
    "    num_iter_total += num_iter\n",
    "    df_data.loc[i, :] = [num_iter_total,\n",
    "                         test_accuracy,\n",
    "                         train_accuracy]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.365333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.465286</td>\n",
       "      <td>0.466016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.549714</td>\n",
       "      <td>0.548937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>0.613143</td>\n",
       "      <td>0.617317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.676143</td>\n",
       "      <td>0.676095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.680857</td>\n",
       "      <td>0.683444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7000</td>\n",
       "      <td>0.700143</td>\n",
       "      <td>0.705365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8000</td>\n",
       "      <td>0.737714</td>\n",
       "      <td>0.743492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9000</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0.82381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.833286</td>\n",
       "      <td>0.839095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11000</td>\n",
       "      <td>0.848429</td>\n",
       "      <td>0.85727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.84327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13000</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>0.858651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.835857</td>\n",
       "      <td>0.846222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.882286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.859429</td>\n",
       "      <td>0.860238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.883571</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18000</td>\n",
       "      <td>0.874429</td>\n",
       "      <td>0.879619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19000</td>\n",
       "      <td>0.885857</td>\n",
       "      <td>0.889286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.889143</td>\n",
       "      <td>0.896651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21000</td>\n",
       "      <td>0.874571</td>\n",
       "      <td>0.883524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22000</td>\n",
       "      <td>0.888429</td>\n",
       "      <td>0.890317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23000</td>\n",
       "      <td>0.884143</td>\n",
       "      <td>0.886143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24000</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.898857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.890286</td>\n",
       "      <td>0.896048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26000</td>\n",
       "      <td>0.888714</td>\n",
       "      <td>0.893857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.884857</td>\n",
       "      <td>0.890762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28000</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>0.904079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29000</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.901667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30000</td>\n",
       "      <td>0.892429</td>\n",
       "      <td>0.893651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34000</td>\n",
       "      <td>0.892429</td>\n",
       "      <td>0.898476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35000</td>\n",
       "      <td>0.894857</td>\n",
       "      <td>0.903889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36000</td>\n",
       "      <td>0.904857</td>\n",
       "      <td>0.909524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37000</td>\n",
       "      <td>0.905286</td>\n",
       "      <td>0.90946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38000</td>\n",
       "      <td>0.901857</td>\n",
       "      <td>0.908857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39000</td>\n",
       "      <td>0.894714</td>\n",
       "      <td>0.902619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40000</td>\n",
       "      <td>0.899429</td>\n",
       "      <td>0.906603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41000</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.904905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42000</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.891952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.906048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44000</td>\n",
       "      <td>0.898857</td>\n",
       "      <td>0.907905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45000</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>0.909444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.898873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47000</td>\n",
       "      <td>0.900143</td>\n",
       "      <td>0.911175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48000</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>0.904683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49000</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.914048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.905857</td>\n",
       "      <td>0.913857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51000</td>\n",
       "      <td>0.911857</td>\n",
       "      <td>0.917111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52000</td>\n",
       "      <td>0.912857</td>\n",
       "      <td>0.917587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53000</td>\n",
       "      <td>0.900714</td>\n",
       "      <td>0.91027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54000</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.91673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55000</td>\n",
       "      <td>0.913714</td>\n",
       "      <td>0.919032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56000</td>\n",
       "      <td>0.905143</td>\n",
       "      <td>0.905841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57000</td>\n",
       "      <td>0.904571</td>\n",
       "      <td>0.910603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58000</td>\n",
       "      <td>0.910429</td>\n",
       "      <td>0.919794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59000</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.914444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.919524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61000</td>\n",
       "      <td>0.909429</td>\n",
       "      <td>0.914746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62000</td>\n",
       "      <td>0.913571</td>\n",
       "      <td>0.917571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63000</td>\n",
       "      <td>0.913429</td>\n",
       "      <td>0.917365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterations test_accuracy train_accuracy\n",
       "0        1000      0.368571       0.365333\n",
       "1        2000      0.465286       0.466016\n",
       "2        3000      0.549714       0.548937\n",
       "3        4000      0.613143       0.617317\n",
       "4        5000      0.676143       0.676095\n",
       "5        6000      0.680857       0.683444\n",
       "6        7000      0.700143       0.705365\n",
       "7        8000      0.737714       0.743492\n",
       "8        9000      0.820857        0.82381\n",
       "9       10000      0.833286       0.839095\n",
       "10      11000      0.848429        0.85727\n",
       "11      12000         0.838        0.84327\n",
       "12      13000      0.849429       0.858651\n",
       "13      14000      0.835857       0.846222\n",
       "14      15000         0.877       0.882286\n",
       "15      16000      0.859429       0.860238\n",
       "16      17000      0.883571          0.884\n",
       "17      18000      0.874429       0.879619\n",
       "18      19000      0.885857       0.889286\n",
       "19      20000      0.889143       0.896651\n",
       "20      21000      0.874571       0.883524\n",
       "21      22000      0.888429       0.890317\n",
       "22      23000      0.884143       0.886143\n",
       "23      24000         0.894       0.898857\n",
       "24      25000      0.890286       0.896048\n",
       "25      26000      0.888714       0.893857\n",
       "26      27000      0.884857       0.890762\n",
       "27      28000      0.899857       0.904079\n",
       "28      29000      0.895286       0.901667\n",
       "29      30000      0.892429       0.893651\n",
       "..        ...           ...            ...\n",
       "33      34000      0.892429       0.898476\n",
       "34      35000      0.894857       0.903889\n",
       "35      36000      0.904857       0.909524\n",
       "36      37000      0.905286        0.90946\n",
       "37      38000      0.901857       0.908857\n",
       "38      39000      0.894714       0.902619\n",
       "39      40000      0.899429       0.906603\n",
       "40      41000         0.892       0.904905\n",
       "41      42000      0.886714       0.891952\n",
       "42      43000           0.9       0.906048\n",
       "43      44000      0.898857       0.907905\n",
       "44      45000      0.899857       0.909444\n",
       "45      46000         0.893       0.898873\n",
       "46      47000      0.900143       0.911175\n",
       "47      48000      0.898286       0.904683\n",
       "48      49000         0.911       0.914048\n",
       "49      50000      0.905857       0.913857\n",
       "50      51000      0.911857       0.917111\n",
       "51      52000      0.912857       0.917587\n",
       "52      53000      0.900714        0.91027\n",
       "53      54000      0.908571        0.91673\n",
       "54      55000      0.913714       0.919032\n",
       "55      56000      0.905143       0.905841\n",
       "56      57000      0.904571       0.910603\n",
       "57      58000      0.910429       0.919794\n",
       "58      59000         0.907       0.914444\n",
       "59      60000      0.914286       0.919524\n",
       "60      61000      0.909429       0.914746\n",
       "61      62000      0.913571       0.917571\n",
       "62      63000      0.913429       0.917365\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x127ad1c50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG5JwF0IINyThLoQQbkjCXQgh3ND/Bz3EaadHGO4vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129514400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_data.iterations, df_data.train_accuracy)\n",
    "plt.plot(df_data.iterations, df_data.test_accuracy);\n",
    "plt.legend(['Train accuracy', 'Test accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural nets: what is going on?\n",
    "\n",
    "* Neural nets are nested functions, where the functions applied to the inputs alternate between two types:\n",
    "    * Linear functions, represented by matrix multiplications\n",
    "    * Non linear \"activation\" functions\n",
    "* Because they are functions, we can represent the neural net making a prediction mathematically. For example, if $X$ is the input, then the prediction $P$ is just: \n",
    "\n",
    "$$ P = D(C(B(A(X, V)), W)) $$\n",
    "\n",
    "where $A$, $B$, $C$, and $D$ are functions and $V$ and $W$ are weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review from last time (Pt. 2)\n",
    "\n",
    "* This prediction is compared to the actual value we were trying to predict, $Y$, and a loss is computed, for example:\n",
    "\n",
    "$$ L = (Y - P) ^ 2 $$\n",
    "\n",
    "* And then the weights are adjusted so that this loss will be reduced:\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key point\n",
    "\n",
    "* This process - of computing derivatives to continually update the weights in a neural net - works because of **the chain rule from calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can do basic neural nets!\n",
    "\n",
    "<img src=\"img/neural_net_check.png\">\n",
    "\n",
    "We got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Going deeper, each individual weight makes a contribution to the loss in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. If the loss of a neural net on a given observation $ X $ is: \n",
    "\n",
    "$$ L = L(D(C(B(A(X, V)), W))) $$\n",
    "\n",
    "we can use the chain rule the explicitly compute the loss with respect to each of the individual weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial C}{\\partial W} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ These are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial A}{\\partial V} * \\frac{\\partial B}{\\partial A} * \\frac{\\partial C}{\\partial B} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ To get all the weight updates to line up correctly, some of these are elementwise multiplications and some of these are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key point:** Each one of these individual computations is actually quite simple. I cover what each one of these is in a blog post [here](http://sethweidman.com/neural_net_post_2) and a SlideShare presentation [here](https://www.slideshare.net/SethHWeidman/neural-nets-from-scratch-72835271)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Read in the MNIST Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  70000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Visualize the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiCo/wfKBnT2y+G31wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ae8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Train the neural net for one epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learn(X, Y, num_iter):\n",
    "    np.random.seed(2)\n",
    "    V = np.random.randn(784, 50)\n",
    "    W = np.random.randn(50, 10)\n",
    "    for j in range(num_iter):\n",
    "        i = np.random.randint(0, num_iter)\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        sum_P = np.sum(P)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y-P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V, W = learn(X_train, Y_train, num_iter=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = predict(X_test, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 91.3 percent\n"
     ]
    }
   ],
   "source": [
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "91.3% accuracy. Not bad for a naive approach with zero tricks. However..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...it's far from optimal: [see here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "\n",
    "<img src='img/MNIST_performance.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building deeper nets\n",
    "* Changing \"learning rates\" / using \"learning rate decay\"\n",
    "* Adding \"dropout\"\n",
    "* ...and that doesn't even cover many of the cutting edge techniques listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And of course, we have to build it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**\"What I cannot build, I cannot understand.\"**\n",
    "\n",
    "--Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before:\n",
    "\n",
    "With a neural net with one hidden layer, one pass through was 25 manually coded steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_This will quickly get unweildy if we add more hidden layers._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New understanding: \"Layers\"\n",
    "\n",
    "To do \"Deep Learning\" you have to stop thinking of neural nets as a series of functions $ L = L(D(C(B(A(X, V)), W)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New understanding: \"Layers\"\n",
    "\n",
    "Instead, we'll think of them as a series of layers:\n",
    "\n",
    "Slide from March talk:\n",
    "\n",
    "<img src='img/neural_net_layers.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll define a `NeuralNetwork` class that defines a neural network as a series of `Layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Props\n",
    "\n",
    "<img src=\"img/andersbll.png\">\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers - and don't forget about that activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        s = sigmoid(x)\n",
    "        return s*(1-s)\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running this neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "layer2 = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net_pass(net, x, y):\n",
    "    pred = net.forwardpass(x)\n",
    "    loss = net.loss(pred, y)\n",
    "    net.backpropogate(loss)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(4)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through every element in the training set: \n",
    "for index in indices:\n",
    "    x = np.array(X_train[index], ndmin=2)\n",
    "    y = np.array(Y_train[index], ndmin=2)\n",
    "    neural_net_pass(nn_mnist, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 82.3 percent\n"
     ]
    }
   ],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A different random weight initialization gave us just 82.3% accuracy this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network\n",
    "\n",
    "#### Turning what we did above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deeper networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=75, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=75, n_out=50, activation_function=sigmoid)\n",
    "hidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_3 = Linear(n_in=50, n_out=25, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=25, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_accuracy_net_layers(layers):\n",
    "    # Define net\n",
    "    net = NeuralNetwork(\n",
    "        layers=layers,\n",
    "        random_seed=2)\n",
    "\n",
    "    accuracy_one_hidden = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    return accuracy_one_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with one hidden layer was 82.3\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with one hidden layer was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with two hidden layers was 90.7\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with two hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with three hidden layers was 88.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with three hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with four hidden layers was 87.9\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      hidden_3,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with four hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**\"Deep Learning\" can help, but we need more \"tricks\" to really get these things working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', self.learning_rate)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    print(\"The accuracy of a network with learning rate\", learning_rate, \"was\", np.round(accuracy_lr * 100.0, 1))\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because backpropogation involves multiplying a value by the derivative of the activation function, gradients (that tell the weights how to update) get smaller and smaller as you go get further from the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sigmoid_deriv_trask.png\">\n",
    "**At most, the gradient can be multiplied by 0.25 at each layer.** More [here](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, \n",
    "                 random_seed, \n",
    "                 learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', \n",
    "                    self.learning_rate/(10.0 ** i))\n",
    "            layer.initialize_weights()\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer is\", \n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accuracy is up to 91.8%, up from 90.7% before. We could also view that as roughly a 10% reduction in error rate. \n",
    "\n",
    "**Having a higher learning rate in earlier layers vs. later layers does help accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights in a neural net are updated according to:\n",
    "\n",
    "$$ W =  W - \\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that this is equivalent to doing gradient descent with each parameter:\n",
    "\n",
    "<img src=\"img/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is analogous to a ball rolling down a hill. \n",
    "\n",
    "Balls rolling down hills have momentum. So, therefore, should our weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum\n",
    "\n",
    "Let's define our weight update $ \\frac{\\partial L}{\\partial W} $ to be $ U_t $. Then, instead of our weight update being $ U_t $ at each time step, it will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ U_t + \\mu * U_{t-1} + \\mu^2 * U_{t-2} + ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\mu$ is a decay parameter between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to, and often described as, increasing your learning rate when your weight updates are going in the same direction, iteration after iteration, and lowering your learning rate when the opposite is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = None\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate, momentum):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate, momentum):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer, and \\n momentum\", momentum, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [1]\n",
    "momentum = [0.5, 0.75, 0.9]\n",
    "for learning_rate in learning_rates:\n",
    "    for m in momentum:\n",
    "        learning_rate_accuracy(learning_rate, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we get a practical lesson: **throwing the \"kitchen sink\" of neural net tricks at a problem in unnecessary at best and actually harmful at worst.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Indeed, learning rate momentum has been shown to work on Recurrent Neural Nets, not fully connected neural nets as we have here. See \n",
    "[Bengio et. al. (2014)](https://arxiv.org/pdf/1212.0901v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout can help prevent neural networks from overfitting. It involves \"dropping\" a portion of the neurons - that is, setting their values to zero - on each forward pass through the network. \n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "This nudges the network toward learning \"redundant representations of its data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, dropout):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.dropout = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_dropout(dropout):\n",
    "    \n",
    "    net = NeuralNetworkDropout(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        dropout = dropout\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with dropout\", dropout, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dropouts = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_dropout(x) for x in dropouts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Adding dropout did _not_ improve accuracy.** Our network was not overfitting in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Last one, just for fun: DropConnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we saw above, the highest performance model on the MNIST data involved \"Drop Connect\", where a portion of the _weights_ in the neural net are set to zero, as opposed to half the _neurons_.\n",
    "\n",
    "[Not in TensorFlow!](https://stackoverflow.com/questions/37135885/dropconnect-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, drop_connect,\n",
    "                 dropout=None):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.drop_connect = drop_connect\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "            setattr(layer, 'drop_connect', self.drop_connect)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    drop_connect = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.activation_input = np.dot(layer_input, \n",
    "                                           drop_connected_weights)\n",
    "        else:\n",
    "            self.activation_input = np.dot(layer_input, self.W)\n",
    "        self.iteration += 1\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.drop_connect = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_drop_connect(drop_connect):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        drop_connect = drop_connect\n",
    "    )\n",
    "\n",
    "    accuracy_lr = train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with drop_connect\", \n",
    "          drop_connect, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "drop_connect_values = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_drop_connect(x) for x in drop_connect_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This network, not being convolutional, is likely _underfitting_ rather than _overfitting_ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a lot of \"tricks\" to improve the training of neural nets. **Now you know not just conceptually what those tricks are doing, but how to implement them.** You may even be able to implement a few of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll examine:\n",
    "* Different activation functions! (here we only used boring ol' sigmoid)\n",
    "* Different weight initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Convolutional layers (yes, from scratch)\n",
    "* Recurrent neural nets (including LSTMs, omg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next steps for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go to [the GitHub repo for this talk](https://github.com/SethHWeidman/neural_net_talks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Star, fork, and contribute!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

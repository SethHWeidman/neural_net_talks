{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Net: Lunch and Learn\n",
    "\n",
    "Seth Weidman    \n",
    "\n",
    "06/15/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we'll cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What we'll cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Reminder of conceptually, what neural nets are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Show that even with this simple understanding, we can start to use them to solve problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Show how to build deeper nets from scratch, using only built-in Python functionality and `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Nets: What is Going On?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='img/neural_net_wtf.png' height=200>\n",
    "\n",
    "What does this actually mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural nets: what is going on?\n",
    "\n",
    "* Neural nets are nested functions, where the functions applied to the inputs alternate between two types:\n",
    "    * Linear functions, represented by matrix multiplications\n",
    "    * Non linear \"activation\" functions\n",
    "* Because they are functions, we can represent the neural net making a prediction mathematically. For example, if $X$ is the input, then the prediction $P$ is just: \n",
    "\n",
    "$$ P = D(C(B(A(X, V)), W)) $$\n",
    "\n",
    "where $A$, $B$, $C$, and $D$ are functions and $V$ and $W$ are weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review from last time (Pt. 2)\n",
    "\n",
    "* This prediction is compared to the actual value we were trying to predict, $Y$, and a loss is computed, for example:\n",
    "\n",
    "$$ L = (Y - P) ^ 2 $$\n",
    "\n",
    "* And then the weights are adjusted so that this loss will be reduced:\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key point\n",
    "\n",
    "* This process - of computing derivatives to continually update the weights in a neural net - works because of **the chain rule from calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can do basic neural nets!\n",
    "\n",
    "<img src=\"img/neural_net_check.png\">\n",
    "\n",
    "We got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Going deeper, each individual weight makes a contribution to the loss in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. If the loss of a neural net on a given observation $ X $ is: \n",
    "\n",
    "$$ L = L(D(C(B(A(X, V)), W))) $$\n",
    "\n",
    "we can use the chain rule the explicitly compute the loss with respect to each of the individual weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Delving into the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial C}{\\partial W} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ These are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial A}{\\partial V} * \\frac{\\partial B}{\\partial A} * \\frac{\\partial C}{\\partial B} * \\frac{\\partial P}{\\partial C} * \\frac{\\partial L}{\\partial P} $$\n",
    "\n",
    "_Note:_ To get all the weight updates to line up correctly, some of these are elementwise multiplications and some of these are matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key point:** Each one of these individual computations is actually quite simple. I cover what each one of these is in a blog post [here](http://sethweidman.com/neural_net_post_2) and a SlideShare presentation [here](https://www.slideshare.net/SethHWeidman/neural-nets-from-scratch-72835271)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Read in the MNIST Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Visualize the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "visualize_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Using this to learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Train the neural net for one epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learn(X, Y, num_iter):\n",
    "    np.random.seed(2)\n",
    "    V = np.random.randn(784, 50)\n",
    "    W = np.random.randn(50, 10)\n",
    "    for j in range(num_iter):\n",
    "        i = np.random.randint(0, num_iter)\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        sum_P = np.sum(P)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y-P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V, W = learn(X_train, Y_train, num_iter=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = predict(X_test, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "91.3% accuracy. Not bad for a naive approach with zero tricks. However..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...it's far from optimal: [see here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "\n",
    "<img src='img/MNIST_performance.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building deeper nets\n",
    "* Changing \"learning rates\" / using \"learning rate decay\"\n",
    "* Adding \"dropout\"\n",
    "* ...and that doesn't even cover many of the cutting edge techniques listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And of course, we have to build it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**\"What I cannot build, I cannot understand.\"**\n",
    "\n",
    "--Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before:\n",
    "\n",
    "With a neural net with one hidden layer, one pass through was 25 manually coded steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_This will quickly get unweildy if we add more hidden layers._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New understanding: \"Layers\"\n",
    "\n",
    "To do \"Deep Learning\" you have to stop thinking of neural nets as a series of functions $ L = L(D(C(B(A(X, V)), W)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New understanding: \"Layers\"\n",
    "\n",
    "Instead, we'll think of them as a series of layers:\n",
    "\n",
    "Slide from March talk:\n",
    "\n",
    "<img src='img/neural_net_layers.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll define a `NeuralNetwork` class that defines a neural network as a series of `Layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Props\n",
    "\n",
    "<img src=\"img/andersbll.png\">\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers - and don't forget about that activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        s = sigmoid(x)\n",
    "        return s*(1-s)\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running this neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "layer2 = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net_pass(net, x, y):\n",
    "    pred = net.forwardpass(x)\n",
    "    loss = net.loss(pred, y)\n",
    "    net.backpropogate(loss)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(4)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through every element in the training set: \n",
    "for index in indices:\n",
    "    x = np.array(X_train[index], ndmin=2)\n",
    "    y = np.array(Y_train[index], ndmin=2)\n",
    "    neural_net_pass(nn_mnist, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A different random weight initialization gave us just 82.3% accuracy this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network\n",
    "\n",
    "#### Turning what we did above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deeper networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_3 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_accuracy_net_layers(layers):\n",
    "    # Define net\n",
    "    net = NeuralNetwork(\n",
    "        layers=layers,\n",
    "        random_seed=2)\n",
    "\n",
    "    accuracy_one_hidden = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    return accuracy_one_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with one hidden layer was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with two hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with three hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      hidden_3,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with four hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**\"Deep Learning\" can help, but we need more \"tricks\" to really get these things working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', self.learning_rate)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    print(\"The accuracy of a network with learning rate\", learning_rate, \"was\", np.round(accuracy_lr * 100.0, 1))\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because backpropogation involves multiplying a value by the derivative of the activation function, gradients (that tell the weights how to update) get smaller and smaller as you go get further from the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sigmoid_deriv_trask.png\">\n",
    "**At most, the gradient can be multiplied by 0.25 at each layer.** More [here](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, \n",
    "                 random_seed, \n",
    "                 learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', \n",
    "                    self.learning_rate/(10.0 ** i))\n",
    "            layer.initialize_weights()\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer is\", \n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accuracy is up to 91.8%, up from 90.7% before. We could also view that as roughly a 10% reduction in error rate. \n",
    "\n",
    "**Having a higher learning rate in earlier layers vs. later layers does help accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights in a neural net are updated according to:\n",
    "\n",
    "$$ W =  W - \\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that this is equivalent to doing gradient descent with each parameter:\n",
    "\n",
    "<img src=\"img/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is analogous to a ball rolling down a hill. \n",
    "\n",
    "Balls rolling down hills have momentum. So, therefore, should our weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum\n",
    "\n",
    "Let's define our weight update $ \\frac{\\partial L}{\\partial W} $ to be $ U_t $. Then, instead of our weight update being $ U_t $ at each time step, it will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ U_t + \\mu * U_{t-1} + \\mu^2 * U_{t-2} + ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\mu$ is a decay parameter between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to, and often described as, increasing your learning rate when your weight updates are going in the same direction, iteration after iteration, and lowering your learning rate when the opposite is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = None\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate, momentum):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate, momentum):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer, and \\n momentum\", momentum, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [1]\n",
    "momentum = [0.5, 0.75, 0.9]\n",
    "for learning_rate in learning_rates:\n",
    "    for m in momentum:\n",
    "        learning_rate_accuracy(learning_rate, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we get a practical lesson: **throwing the \"kitchen sink\" of neural net tricks at a problem in unnecessary at best and actually harmful at worst.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Indeed, learning rate momentum has been shown to work on Recurrent Neural Nets, not fully connected neural nets as we have here. See \n",
    "[Bengio et. al. (2014)](https://arxiv.org/pdf/1212.0901v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout can help prevent neural networks from overfitting. It involves \"dropping\" a portion of the neurons - that is, setting their values to zero - on each forward pass through the network. \n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "This nudges the network toward learning \"redundant representations of its data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, dropout):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.dropout = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_dropout(dropout):\n",
    "    \n",
    "    net = NeuralNetworkDropout(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        dropout = dropout\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with dropout\", dropout, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dropouts = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_dropout(x) for x in dropouts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Adding dropout did _not_ improve accuracy.** Our network was not overfitting in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Last one, just for fun: DropConnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we saw above, the highest performance model on the MNIST data involved \"Drop Connect\", where a portion of the _weights_ in the neural net are set to zero, as opposed to half the _neurons_.\n",
    "\n",
    "[Not in TensorFlow!](https://stackoverflow.com/questions/37135885/dropconnect-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, drop_connect,\n",
    "                 dropout=None):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.drop_connect = drop_connect\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "            setattr(layer, 'drop_connect', self.drop_connect)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    drop_connect = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.activation_input = np.dot(layer_input, \n",
    "                                           drop_connected_weights)\n",
    "        else:\n",
    "            self.activation_input = np.dot(layer_input, self.W)\n",
    "        self.iteration += 1\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.drop_connect = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_drop_connect(drop_connect):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        drop_connect = drop_connect\n",
    "    )\n",
    "\n",
    "    accuracy_lr = train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with drop_connect\", \n",
    "          drop_connect, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "drop_connect_values = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_drop_connect(x) for x in drop_connect_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This network, not being convolutional, is likely _underfitting_ rather than _overfitting_ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a lot of \"tricks\" to improve the training of neural nets. **Now you know not just conceptually what those tricks are doing, but how to implement them.** You may even be able to implement a few of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll examine:\n",
    "* Different activation functions! (here we only used boring ol' sigmoid)\n",
    "* Different weight initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Convolutional layers (yes, from scratch)\n",
    "* Recurrent neural nets (including LSTMs, omg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next steps for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go to [the GitHub repo for this talk](https://github.com/SethHWeidman/neural_net_talks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Star, fork, and contribute!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
